{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBfys4iyIAWw"
      },
      "outputs": [],
      "source": [
        "# Create an alias with the as keyword while importing\n",
        "# Now you can refer to the Pandas package as pd instead of pandas\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r9fCBVHHe42"
      },
      "source": [
        "# **Previous Content**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AnbG-EIZ0Hq"
      },
      "source": [
        "# Read CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED8PEKKDZzj7"
      },
      "outputs": [],
      "source": [
        "# Load the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "print(sales) # If the dataframe has many rows, Pandas will only return the first 5 rows and the last 5 rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6XwTokSbo4B"
      },
      "outputs": [],
      "source": [
        "# The head() method returns the headers and a specified number of rows, starting from the top\n",
        "print(sales.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vcY_F5vBbyAB"
      },
      "outputs": [],
      "source": [
        "# Use info() to get more information about the dataframe\n",
        "print(sales.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q10CuVWe4kv"
      },
      "outputs": [],
      "source": [
        "# sales['Price'].describe() # use the describe() to get a statistical description of a column\n",
        "sales.describe() # get a statistical description of the entire dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "909IFyzofp7G"
      },
      "source": [
        "# Correct Wrong Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmASstk0fs61"
      },
      "outputs": [],
      "source": [
        "# Suppose after double checking, we conclude the price in Row 0 should not be 2099.00\n",
        "# One way to fix wrong values is to replace them with correct values\n",
        "sales.loc[0, 'Price'] = 20.99 # loc locates values by labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa4od5RtifzM"
      },
      "source": [
        "You may not be able to replace the wrong data one by one for big datasets. To replace wrong data for larger data sets you can create some rules. For example, you can set some boundaries for legal values, and replace any values that are outside of the boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWiHS1CXjHdC"
      },
      "outputs": [],
      "source": [
        "sales.loc[sales['Price'] > 15, 'Price'] = 15\n",
        "print(sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bCJu3UukDoP"
      },
      "outputs": [],
      "source": [
        "sales['Price'] > 15  # returns a pandas series of boolean values that indicate which rows satisfy the condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8JKj9-3jsWC"
      },
      "outputs": [],
      "source": [
        "sales.loc[sales['Price'] > 15] # returns a pandas dataframe with rows corresponding to the True values in the previous pandas series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5gy5ZQOEkVk"
      },
      "outputs": [],
      "source": [
        "sales.loc[sales['Price'] > 15, 'Price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9PNFjIOpOAH"
      },
      "source": [
        "# Clean Empty Cells"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uHS-qoJpb8F"
      },
      "outputs": [],
      "source": [
        "# One way to deal with empty cells is to remove rows that contain empty cells\n",
        "# This is usually OK if the dataset is big and removing a few rows will not have a big impact on the analysis results\n",
        "sales_drop_na = sales.dropna()\n",
        "print(sales_drop_na) # sales_drop_na does not have the row with index 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUlap9HNqB7b"
      },
      "outputs": [],
      "source": [
        "print(sales) # By default, dropna() returns a new dataframe and will not change the original dataframe\n",
        "# So the row with index 6 still exists in df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVAmJZ18qdpz"
      },
      "outputs": [],
      "source": [
        "sales.dropna(inplace = True)\n",
        "print(sales) # dropna(inplace = True) will NOT return a new DataFrame. Instead, it will remove all rows containing NULL values from the original dataframe\n",
        "# The row with index 6 is now removed from the sales dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqXrB4B7rVIo"
      },
      "outputs": [],
      "source": [
        "# Reload the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "print(sales) # sales now has a null value in the Price column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEwjp6IEu6fC"
      },
      "outputs": [],
      "source": [
        "sales.dropna(subset=['Price'], inplace = True) # Remove rows with a NULL value in the Price column\n",
        "print(sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al9L4lAFvICM"
      },
      "outputs": [],
      "source": [
        "# Reload the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "print(sales) # sales has a null value in the Price column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xzBNn-3sGjs"
      },
      "outputs": [],
      "source": [
        "# The following code looks correct but it only returns a pandas series with the sixth position replaced by 20\n",
        "sales[\"Price\"].fillna(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gMev3OCsUMN"
      },
      "outputs": [],
      "source": [
        "print(sales) # sales still has the null value in the Price column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLf2VdvNrHXe"
      },
      "outputs": [],
      "source": [
        "# Replace NULL values in the Price column with the number 20\n",
        "sales[\"Price\"].fillna(20, inplace = True) # We need to set inplace = True so we are modifying df\n",
        "print(sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ik9OyZvkuJgK"
      },
      "outputs": [],
      "source": [
        "# Reload the dataframe and follow the suggestion in the warning message\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "sales.fillna({\"Price\": 20}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qonqL-AZuQOf"
      },
      "outputs": [],
      "source": [
        "# Reload the dataframe and follow the suggestion in the warning message\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "sales[\"Price\"] = sales[\"Price\"].fillna(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1o4G3wixcYW"
      },
      "source": [
        "# Handle Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19OLJPw1xkbo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# Use duplicated() to discover duplicates\n",
        "# duplicated() returns a Boolean value for each row, i.e., True for every row that is a duplicate, othwerwise False\n",
        "print(df.duplicated())\n",
        "# Note that Rows with index 8 and 9 are the same\n",
        "# Row 8 is not a duplicate, and Row 9 is a duplicate\n",
        "# Documentation at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html: the keep parameter by default is 'first'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKC4W6hUyNzp"
      },
      "outputs": [],
      "source": [
        "# Use drop_duplicates() to remove duplicates\n",
        "df.drop_duplicates(inplace = True) # We set inplace = True to make sure that the method does NOT return a new dataframe, but it will remove all duplicates from the original dataframe\n",
        "print(df)\n",
        "# Note that Row 8 is kept, and Row 9 is removed\n",
        "# Documentation at https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html: the keep parameter by default is 'first'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBkKMNB2zwH8"
      },
      "outputs": [],
      "source": [
        "# Reload the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "print(sales) # sales has a null value in the Price column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2AKdoDk06dY"
      },
      "outputs": [],
      "source": [
        "# To drop duplicated rows based on one column's value in a pandas DataFrame, you can use the drop_duplicates() and specify the column name using the subset parameter\n",
        "# By default, this method keeps the first occurrence of each duplicated row and drops the rest\n",
        "sales.drop_duplicates(subset=['Transaction_ID']) # By default, inplace = False\n",
        "# sales.drop_duplicates(subset=['Transaction_ID'], inplace = True)\n",
        "# print(sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTwgsaGsz1C8"
      },
      "outputs": [],
      "source": [
        "# Reload the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# To drop duplicated rows based on one column's value in a pandas DataFrame, you can use the drop_duplicates() and specify the column name using the subset parameter\n",
        "# By default, this method keeps the first occurrence of each duplicated row and drops the rest (i.e.,  keep='first')\n",
        "sales.drop_duplicates(subset=['Product_ID'], keep='first')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0dmATzK0UAH"
      },
      "outputs": [],
      "source": [
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# Use keep='last' to drop duplicates except for the last occurrence\n",
        "sales.drop_duplicates(subset=['Product_ID'], keep='last')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lasD1g1K0g6W"
      },
      "outputs": [],
      "source": [
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# Use keep=False to drop all duplicates\n",
        "sales.drop_duplicates(subset=['Product_ID'], keep=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cf7cJCez_ME"
      },
      "source": [
        "**Practice 1: Write code to get all the unique pairs of Customer_ID and Product_ID from the \"sales\" dataframe**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hov6KIa4w_AZ"
      },
      "source": [
        "# Clean Date Format (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7yU0fumt8oX"
      },
      "outputs": [],
      "source": [
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# Pandas has a to_datetime() method for converting cell values to dates\n",
        "sales['Date'] = pd.to_datetime(sales['Date'], format='mixed', dayfirst =False)\n",
        "print(sales)\n",
        "# Documentation at https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html:\n",
        "# Use format='mixed' to infer the format for each element individually. This is risky, so we set dayfirst = False which indicates that we don't prefer to parse with day first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEQNydLdHmXT"
      },
      "source": [
        "# **New Content**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HEoooDoDfPU"
      },
      "source": [
        "# Data Analytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrzl93qoMyR_"
      },
      "outputs": [],
      "source": [
        "# Load the CSV into a dataframe called sales\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "print(sales)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H89JoGYBO-mP"
      },
      "outputs": [],
      "source": [
        "# For each transaction, transaction amount = quantity * price\n",
        "# Add a column for transaction amount\n",
        "sales['TransactionAmount'] = sales['Quantity'] * sales['Price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFRMJdCXFIf"
      },
      "outputs": [],
      "source": [
        "# What is the average transaction amount?\n",
        "# Automatically ignored null values when calculating the mean\n",
        "sales['TransactionAmount'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z15AxGlCXIAs"
      },
      "outputs": [],
      "source": [
        "# What is the total transaction amount?\n",
        "# Automatically ignored null values when calculating the sum\n",
        "sales['TransactionAmount'].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnTGO9QcyW4d"
      },
      "outputs": [],
      "source": [
        "sales['TransactionAmount'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtwoP-EFRDue"
      },
      "outputs": [],
      "source": [
        "# Use .groupby() to group your data and execute functions (e.g., sum) on these groups\n",
        "sales.groupby('Customer_ID').sum() # .sum() calculates the total of numeric columns and concatenates the string columns\n",
        "# Row index 6 has null values in Price and TransactionAmount columns\n",
        "# The null values in Price and TransactionAmount are ignored\n",
        "# Other numeric columns (Transaction_ID, Quantity) in row index 6 are not ignored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9EUiluBZiRT"
      },
      "outputs": [],
      "source": [
        "# The null values can be confusing and intervene with our analysis\n",
        "# The best practice is to clean up the null values (and other problems) before analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNtgzvAIZgII"
      },
      "source": [
        "**Step 1 (Revision - Handle Duplicates): Remove duplicated rows in the sales dataframe with keeping the first occurence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgDYR6HQSOoo"
      },
      "outputs": [],
      "source": [
        "# Load the CSV into a dataframe\n",
        "sales = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/sales_data.csv')\n",
        "# Clean the sales data\n",
        "# Step 1: Remove duplicated rows\n",
        "sales.drop_duplicates(inplace = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWW3-wLyMVqM"
      },
      "source": [
        "**Step 2 (Revision: Correct Wrong Values): Change the price of Product P001 to 20.99 and recalculate transaction amount (Quantity * Price)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duTHaWUISTlO"
      },
      "outputs": [],
      "source": [
        "# Step 2: Correct wrong values\n",
        "sales.loc[sales['Product_ID'] == 'P001', 'Price'] = 20.99\n",
        "sales['TransactionAmount'] = sales['Quantity'] * sales['Price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0AQlPUJLIt-"
      },
      "source": [
        "**Step 3 (Revision: Fill Null Values): Change the null value of Price to the weighted price of all other products in the sales table (i.e., total transaction amount of all the products divided by total quantity of all the products) and recalculate transaction amount**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXNlGwK7Sg_d"
      },
      "outputs": [],
      "source": [
        "# Step 3: Clean null values\n",
        "weighted_price = sales['TransactionAmount'].sum() / sales['Quantity'].sum()\n",
        "sales['Price'] = sales['Price'].fillna(weighted_price)\n",
        "# Recalculate Transaction Amount\n",
        "sales['TransactionAmount'] = sales['Quantity'] * sales['Price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX9ED8rnT8dK"
      },
      "outputs": [],
      "source": [
        "# We can specify to only perform .sum() on the TransactionAmount column\n",
        "total_amount_per_customer = sales.groupby('Customer_ID')['TransactionAmount'].sum()\n",
        "print(total_amount_per_customer)\n",
        "print()\n",
        "print(type(total_amount_per_customer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa7jM1abERwk"
      },
      "outputs": [],
      "source": [
        "# We can specify to perform .sum() on more than one column\n",
        "sales.groupby('Customer_ID')[['TransactionAmount', 'Transaction_ID']].sum() # summing transaction ID is not meaningful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kgqnmKFWrwi"
      },
      "source": [
        "**Practice 2: Write code to answer the following question: for each product, what is its average price weighted by transaction quantity?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMIe7fXbDlPn"
      },
      "outputs": [],
      "source": [
        "# Use .value_counts() to count the number of transactions for each customer\n",
        "sales['Customer_ID'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm1pY_DDRG48"
      },
      "source": [
        "**Practice 3: Write code to count the number of transactions for each pair of customer and product**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7dMBRW_E4vG"
      },
      "outputs": [],
      "source": [
        "# Use .nlargest() to get the top five customers with the largest total amount\n",
        "total_amount_per_customer.nlargest(5) # total_amount_per_customer is a pandas series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gvk0-YjrShQK"
      },
      "outputs": [],
      "source": [
        "# Use .idxmax() to return the label of first occurrence of maximum value\n",
        "total_amount_per_customer.idxmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ6ywiPHRw7N"
      },
      "source": [
        "**Practice 4: Use .nlargest(), .nsmallest(), .idxmax(), and .idxmin() to write code to get the product with the highest and lowest average price weighted by transaction quantity**\n",
        "\n",
        "**Hint 1: Use your answer from Practice 2**\n",
        "\n",
        "**Hint 2: Read resources here: https://tutorialsinhand.com/Articles/pandas-dataframe---nsmallest-and-nlargest.aspx and https://proclusacademy.com/blog/quicktip/pandas-idxmin-idxmax/**\n",
        "\n",
        "**The second resource also talks about having repeated max and min values**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7AnbG-EIZ0Hq",
        "909IFyzofp7G",
        "c9PNFjIOpOAH",
        "I1o4G3wixcYW",
        "Hov6KIa4w_AZ",
        "3HEoooDoDfPU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
